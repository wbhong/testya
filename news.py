import json
import requests
from bs4 import BeautifulSoup
import schedule
import time
from datetime import datetime

# --- ì„¤ì • ---
SEARCH_QUERY = "íŒŒì´ì¬ ì›¹ ìŠ¤í¬ë˜í•‘"  # ê²€ìƒ‰í•  í‚¤ì›Œë“œ
NAVER_NEWS_URL = f"https://search.naver.com/search.naver?where=news&sm=tab_pge&query={SEARCH_QUERY}&sort=1"
# sort=1ì€ ìµœì‹ ìˆœ ì •ë ¬ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
# -----------------

def scrape_news():
    """
    ë„¤ì´ë²„ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    print("=" * 50)
    print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‹œì‘...")
    
    try:
        # ë´‡ìœ¼ë¡œ ì¸ì‹ë˜ì§€ ì•Šë„ë¡ User-Agent ì„¤ì •
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(NAVER_NEWS_URL, headers=headers)
        response.raise_for_status() # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ë‰´ìŠ¤ í•­ëª©ì„ í¬í•¨í•˜ëŠ” ì»¨í…Œì´ë„ˆ ì°¾ê¸° (ë„¤ì´ë²„ ë‰´ìŠ¤ í˜„ì¬ êµ¬ì¡° ê¸°ì¤€)
        news_items = soup.select('div.news_area')
        
        if not news_items:
            print("â— ìŠ¤í¬ë˜í•‘í•  ë‰´ìŠ¤ í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì„ íƒì í™•ì¸ í•„ìš”)")
            print("-" * 50)
            return

        summarized_data = []

        for item in news_items:
            # 1. ì œëª© ë° ë§í¬ ì¶”ì¶œ
            title_link = item.select_one('a.news_tit')
            title = title_link.get('title') if title_link else "ì œëª© ì—†ìŒ"
            url = title_link.get('href') if title_link else "#"

            # 2. ì–¸ë¡ ì‚¬ ì¶”ì¶œ
            source_tag = item.select_one('a.info.press')
            source = source_tag.text.strip() if source_tag else "ì–¸ë¡ ì‚¬ ì •ë³´ ì—†ìŒ"
            
            # 3. ì—…ë¡œë“œ ì¼ì ì¶”ì¶œ (ì—…ë¡œë“œ ì¼ì íƒœê·¸ê°€ ì •í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì–´, ë‰´ìŠ¤ ê·¸ë£¹ ì „ì²´ì—ì„œ ì°¾ìŒ)
            date_tag = item.select_one('.info_group .info:not(.press)')
            upload_date = date_tag.text.strip() if date_tag and "ì‹œê°„" not in date_tag.text and "ë¶„" not in date_tag.text else "ë‚ ì§œ ì •ë³´ ì—†ìŒ"
            # ë„¤ì´ë²„ëŠ” 'nì‹œê°„ ì „' ê°™ì€ ìƒëŒ€ ì‹œê°„ì„ ë§ì´ ì‚¬ìš©í•˜ë¯€ë¡œ, ì •í™•í•œ ë‚ ì§œê°€ ì•„ë‹ˆë©´ 'ë‚ ì§œ ì •ë³´ ì—†ìŒ' ì²˜ë¦¬

            summarized_data.append({
                'title': title,
                'source': source,
                'date': upload_date,
                'url': url
            })

        print(f"âœ”ï¸ ì´ {len(summarized_data)}ê°œì˜ ë‰´ìŠ¤ ì •ë³´ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\n")
        
        # ìš”ì•½ ì •ë³´ ì¶œë ¥
        print("--- ë‰´ìŠ¤ ìš”ì•½ ì •ë³´ ---")
        for idx, news in enumerate(summarized_data, 1):
            print(f"[{idx}]")
            print(f"  ğŸ“Œ ì œëª©: {news['title']}")
            print(f"  ğŸ¢ ì–¸ë¡ ì‚¬: {news['source']}")
            print(f"  ğŸ“… ì—…ë¡œë“œ ì¼ì: {news['date']}")
            print(f"  ğŸ”— ë§í¬: {news['url']} (ì´ ì£¼ì†Œë¥¼ ë³µì‚¬í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ í´ë¦­í•˜ì—¬ ì´ë™)")
            print("-" * 20)
            
    except requests.exceptions.RequestException as e:
        print(f"âŒ ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë˜í•‘ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    print("=" * 50)


def main():
    """
    ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ ì‘ì—…ì„ ì˜ˆì•½í•˜ê³  ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”” í‚¤ì›Œë“œ: '{SEARCH_QUERY}' ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("   ë§¤ 1ì‹œê°„ë§ˆë‹¤ ì‘ì—…ì„ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # ì²˜ìŒ í•œ ë²ˆ ì¦‰ì‹œ ì‹¤í–‰
    scrape_news()
    
    # ë§¤ 1ì‹œê°„ë§ˆë‹¤ scrape_news í•¨ìˆ˜ ì‹¤í–‰ ì˜ˆì•½
    schedule.every(1).hour.do(scrape_news)
    # ë‹¤ë¥¸ ì˜ˆì‹œ: schedule.every(5).minutes.do(scrape_news) # 5ë¶„ë§ˆë‹¤
    # ë‹¤ë¥¸ ì˜ˆì‹œ: schedule.every().day.at("10:30").do(scrape_news) # ë§¤ì¼ 10ì‹œ 30ë¶„ì—
    
    while True:
        schedule.run_pending()
        time.sleep(1) # CPU ë¶€í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ 1ì´ˆ ëŒ€ê¸°

if __name__ == "__main__":
    main()
